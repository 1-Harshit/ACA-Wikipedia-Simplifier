{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6St-pItmiAvN"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibrHJDfFtzPQ",
        "outputId": "86a6431d-401a-4176-d57a-60eb17eed857"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import random as rand\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "EPISODES = 1\n",
        "\n",
        "class env:\n",
        "  def __init__(self):\n",
        "    self.state_size = 4\n",
        "    self.action_size = 2\n",
        "    self.num_agent_1 = 0\n",
        "    self.num_agent_2 = 0\n",
        "    self.num_agent_3 = 0\n",
        "    self.interaction_history = []\n",
        "    self.coop = 0\n",
        "    self.defect = 0\n",
        "    self.face_coop = 0\n",
        "    self.face_defect = 0\n",
        "    self.state = [self.coop,self.defect,self.face_coop,self.face_defect]\n",
        "  \n",
        "  def reset(self):\n",
        "    self.num_agent1 = rand.randint(1,100)\n",
        "    self.num_agent2 = rand.randint(1,100)\n",
        "    self.num_agent3 = rand.randint(1,100)\n",
        "    self.interaction_history = np.zeros(self.num_agent3)\n",
        "    self.coop = rand.randint(1,100)\n",
        "    self.defect = rand.randint(1,100)\n",
        "    self.face_coop = rand.randint(1,100)\n",
        "    self.face_defect = rand.randint(1,100)\n",
        "    proportion = self.num_agent3/(self.num_agent1+self.num_agent2+self.num_agent3)\n",
        "    for i in range(int(self.coop*proportion)):\n",
        "      k = rand.randint(1,self.num_agent3)-1\n",
        "      if self.interaction_history[k]!=1:\n",
        "        self.interaction_history[k]=1\n",
        "    for i in range(int(self.defect*proportion)):\n",
        "      k = rand.randint(1,self.num_agent3)-1\n",
        "      if self.interaction_history[k]!=-1:\n",
        "        self.interaction_history[k]=-1\n",
        "    self.state = [self.coop,self.defect,self.face_coop,self.face_defect]\n",
        "    return self.state\n",
        "\n",
        "  def step(self,action):\n",
        "    if action==0:\n",
        "      action = -1\n",
        "    encounter = rand.randint(1,self.num_agent1+self.num_agent2+self.num_agent3)\n",
        "    partner_does = 0\n",
        "    if encounter<self.num_agent1:\n",
        "      partner_does = 1\n",
        "    if encounter>self.num_agent1 and encounter<self.num_agent2+self.num_agent1:\n",
        "      partner_does = -1\n",
        "    if encounter>self.num_agent1 + self.num_agent2:\n",
        "      partner_does = self.interaction_history[encounter-self.num_agent1-self.num_agent2-1]\n",
        "      self.interaction_history[encounter-self.num_agent1-self.num_agent2-1] = action\n",
        "    if partner_does == 0:\n",
        "      partner_does = rand.choice([-1,1])\n",
        "    reward = 0 #initialize reward 0\n",
        "    if partner_does == 1:\n",
        "      self.face_coop+=1\n",
        "      if action == 1:\n",
        "        reward = 1\n",
        "      if action == -1:\n",
        "        reward = 2\n",
        "    if partner_does == -1:\n",
        "      self.face_defect+=1\n",
        "      if action == 1:\n",
        "        reward = 0\n",
        "      if action == -1:\n",
        "        reward = 0.5\n",
        "    if action == 1:\n",
        "      self.coop+=1\n",
        "    else:\n",
        "      self.defect+=1\n",
        "    self.num_agent1 = int(1.5*self.num_agent1)\n",
        "    self.num_agent2 = int(0.9*self.num_agent2)\n",
        "    original = self.num_agent3\n",
        "    self.num_agent3 = int(1.25*self.num_agent3)\n",
        "    diff = self.num_agent3 - original\n",
        "    for i in range(diff):\n",
        "      self.interaction_history = np.append(self.interaction_history,0)\n",
        "\n",
        "\n",
        "    self.state = [self.coop,self.defect,self.face_coop,self.face_defect]\n",
        "    return self.state, reward\n",
        "    \n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "        self.target_model = self._build_model()\n",
        "        self.update_target_model()\n",
        "\n",
        "   \n",
        "    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):\n",
        "        error = y_true - y_pred\n",
        "        cond  = K.abs(error) <= clip_delta\n",
        "\n",
        "        squared_loss = 0.5 * K.square(error)\n",
        "        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)\n",
        "\n",
        "        return K.mean(tf.where(cond, squared_loss, quadratic_loss))\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss=self._huber_loss,\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        # copy weights from model to target_model\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def memorize(self, state, action, reward, next_state):\n",
        "        self.memory.append((state, action, reward, next_state))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return rand.choice([0,1])\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state in minibatch:\n",
        "            target = self.model.predict(state)\n",
        "            t = self.target_model.predict(next_state)[0]\n",
        "            target[0][action] = reward + self.gamma * np.amax(t)\n",
        "            # target[0][action] = reward + self.gamma * t[np.argmax(a)]\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = env()\n",
        "    state_size = 4\n",
        "    action_size = 2\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    batch_size = 16\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(50):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.memorize(state, action, reward, next_state)\n",
        "            print(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        \n",
        "    state = env.reset()\n",
        "    string = input(\"Run simulation with trained agvent?\")\n",
        "    if string == \"yes\":\n",
        "      for i in range (50):\n",
        "        print(env.num_agent1,env.num_agent2,env.num_agent3,env.interaction_history)\n",
        "        action = agent.act(state)\n",
        "        print(action)\n",
        "        next_state, reward = env.step(action)\n",
        "        print(reward)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        agent.memorize(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[99 39  8 77]] 0 0.5 [[99 40  8 78]]\n",
            "[[99 40  8 78]] 1 0 [[100  40   8  79]]\n",
            "[[100  40   8  79]] 0 0.5 [[100  41   8  80]]\n",
            "[[100  41   8  80]] 1 0 [[101  41   8  81]]\n",
            "[[101  41   8  81]] 1 1 [[102  41   9  81]]\n",
            "[[102  41   9  81]] 1 0 [[103  41   9  82]]\n",
            "[[103  41   9  82]] 0 2 [[103  42  10  82]]\n",
            "[[103  42  10  82]] 1 0 [[104  42  10  83]]\n",
            "[[104  42  10  83]] 1 1 [[105  42  11  83]]\n",
            "[[105  42  11  83]] 0 2 [[105  43  12  83]]\n",
            "[[105  43  12  83]] 1 1 [[106  43  13  83]]\n",
            "[[106  43  13  83]] 0 2 [[106  44  14  83]]\n",
            "[[106  44  14  83]] 0 2 [[106  45  15  83]]\n",
            "[[106  45  15  83]] 0 2 [[106  46  16  83]]\n",
            "[[106  46  16  83]] 1 1 [[107  46  17  83]]\n",
            "[[107  46  17  83]] 0 2 [[107  47  18  83]]\n",
            "[[107  47  18  83]] 0 2 [[107  48  19  83]]\n",
            "[[107  48  19  83]] 0 2 [[107  49  20  83]]\n",
            "[[107  49  20  83]] 0 2 [[107  50  21  83]]\n",
            "[[107  50  21  83]] 0 2 [[107  51  22  83]]\n",
            "[[107  51  22  83]] 0 2 [[107  52  23  83]]\n",
            "[[107  52  23  83]] 0 2 [[107  53  24  83]]\n",
            "[[107  53  24  83]] 1 1 [[108  53  25  83]]\n",
            "[[108  53  25  83]] 0 2 [[108  54  26  83]]\n",
            "[[108  54  26  83]] 0 2 [[108  55  27  83]]\n",
            "[[108  55  27  83]] 0 2 [[108  56  28  83]]\n",
            "[[108  56  28  83]] 1 1 [[109  56  29  83]]\n",
            "[[109  56  29  83]] 0 2 [[109  57  30  83]]\n",
            "[[109  57  30  83]] 0 2 [[109  58  31  83]]\n",
            "[[109  58  31  83]] 1 1 [[110  58  32  83]]\n",
            "[[110  58  32  83]] 0 2 [[110  59  33  83]]\n",
            "[[110  59  33  83]] 1 1 [[111  59  34  83]]\n",
            "[[111  59  34  83]] 0 2 [[111  60  35  83]]\n",
            "[[111  60  35  83]] 1 1 [[112  60  36  83]]\n",
            "[[112  60  36  83]] 0 2 [[112  61  37  83]]\n",
            "[[112  61  37  83]] 0 2 [[112  62  38  83]]\n",
            "[[112  62  38  83]] 1 1 [[113  62  39  83]]\n",
            "[[113  62  39  83]] 1 1 [[114  62  40  83]]\n",
            "[[114  62  40  83]] 0 2 [[114  63  41  83]]\n",
            "[[114  63  41  83]] 0 2 [[114  64  42  83]]\n",
            "[[114  64  42  83]] 0 2 [[114  65  43  83]]\n",
            "[[114  65  43  83]] 0 2 [[114  66  44  83]]\n",
            "[[114  66  44  83]] 1 1 [[115  66  45  83]]\n",
            "[[115  66  45  83]] 0 2 [[115  67  46  83]]\n",
            "[[115  67  46  83]] 1 1 [[116  67  47  83]]\n",
            "[[116  67  47  83]] 0 2 [[116  68  48  83]]\n",
            "[[116  68  48  83]] 1 1 [[117  68  49  83]]\n",
            "[[117  68  49  83]] 0 2 [[117  69  50  83]]\n",
            "[[117  69  50  83]] 1 1 [[118  69  51  83]]\n",
            "[[118  69  51  83]] 1 1 [[119  69  52  83]]\n"
          ]
        }
      ]
    }
  ]
}